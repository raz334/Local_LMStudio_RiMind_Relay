RiMind Relay that allows Local AI Models to run using LM Studio.
It Lets Rimind Redirect all of it's AI calls to the LMStudio and communicate just like it would for the google API.

Requires: Rimworld, RiMind, LMStudio.

RiMind: https://steamcommunity.com/sharedfiles/filedetails/?id=3562373405
LMStudio: https://lmstudio.ai/

Setup : 
1. Inside Rimworld in the RiMind Basic Settings: Choose Local LLM, Choose Ollama, set the http://localhost:11434 , Leave model name on default llama3.1
2. Ensure LMStudio Server is active and set to run on http://127.0.0.1:1234
3. Choose Your AI model and load it, Must be a chat model no Embedding, but you can run one for use alongside of RimWorldAI I created while doing and tested.
4. Run The Executable, 
5. A Command black window comes up this is the relay that will the magic all work don't close it, keep it up while running the game.
6. ENJOY !

If you choose a good AI model now you can really bring life to your characters and not just plain old broken
and devoid of spirit that charges you for every Google AI API Sent.

** WARNING, You Need a powerful System to Run both Rimworld and The AI models Side by Side. LMStudio helps alot with that but has it's limits.
You may experience Lag and Slow downs espically when content is being generated. 
**This is Entirely your doing Choose the model based on your system specs and Tolerances for Lag.
I recommend no less than a 3B model and no more than an 8B model, but you can chose outside of those ranges.

LMStudio has a Model discovery and download automatically managing it for you.
